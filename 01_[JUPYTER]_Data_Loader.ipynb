{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 모델을 학습시키기 위해 준비되어야 할 4가지 요소\n",
    "\n",
    "**1. 데이터**\n",
    "2. 모델\n",
    "3. Loss function (손실함수, 목적함수, objective function 등으로 불려요): 정답과 모델의 예측값을 어떤 식으로 비교할지 결정해주는 함수\n",
    "4. optimizer: gradient descent를 해줄 애. 즉, 모델의 파라미터를 어느 방향으로 조금 수정할지 결정하고 수정해주는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in Ipython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "\n",
    "data_root = './data'\n",
    "if not os.path.exists(data_root):\n",
    "    os.mkdir(data_root)\n",
    "    \n",
    "def dataset_downloader(filename):\n",
    "    \"\"\"데이터셋 파일이 없으면 다운로드 합니다.\"\"\"\n",
    "    dest_dir = os.path.join(data_root, filename)\n",
    "    if not os.path.exists(dest_dir):\n",
    "        print('다운로드 시도 중 : ', filename)\n",
    "        filename, _ = urlretrieve(url + filename, dest_dir)\n",
    "        print(filename, ' 다운로드 완료!')\n",
    "    else:\n",
    "        print(dest_dir, ' 이미 있습니다.')\n",
    "    \n",
    "    return dest_dir\n",
    "\n",
    "train_filename = dataset_downloader('notMNIST_large.tar.gz')\n",
    "test_filename = dataset_downloader('notMNIST_small.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다운로드한 데이터 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(1000)\n",
    "\n",
    "def data_extract(filename):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    \n",
    "    if os.path.isdir(root):\n",
    "        print('{} 이미 있습니다 - {} 는 추출을 건너뜁니다.'.format(root, filename))\n",
    "    else:\n",
    "        print('{} 에서 데이터를 추출합니다.'.format(root))\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    \n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('{} folders 기대했는데, {} 개가 있네요.'.format(num_classes, len(data_folders)))\n",
    "    \n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "train_folders = data_extract(train_filename)\n",
    "test_folders = data_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 글자 데이터를 로드해서 pickle 파일 형태로 저장하기\n",
    "\n",
    "주의: 오래 걸립니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder):\n",
    "    \"\"\"한 글자 클래스 데이터를 로드합니다.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    \n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth  # image 픽셀값의 범위를 0~1로 만들어줍니다.\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('이미지가 이상한 크기인데요?: {}'.format(str(image_data.shape)))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('{} - skip'.format(e))\n",
    "    \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    print('전체 데이터셋 모양은 다음과 같습니다:', dataset.shape)\n",
    "    \n",
    "    return dataset\n",
    "        \n",
    "def make_pickle(data_folders):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename):\n",
    "            print('{} 이미 있습니다 - pickling을 건너뜁니다.'.format(set_filename))\n",
    "            continue\n",
    "        print('Pickling {}'.format(set_filename))\n",
    "        dataset = load_letter(folder)\n",
    "        with open(set_filename, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = make_pickle(train_folders)\n",
    "test_datasets = make_pickle(test_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 예시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in tqdm(range(len(train_datasets))):\n",
    "    set_filename = train_datasets[i]\n",
    "    with open(set_filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    images.append(dataset[1])\n",
    "print(np.shape(images))\n",
    "\n",
    "Row = 2\n",
    "Column = 5\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(Row, Column, i+1)\n",
    "    plt.title('Label = {}'.format(os.path.basename(train_datasets[i])[0]))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이닝셋, 테스트셋 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== 트레이닝셋을 위한 데이터들 ====\")\n",
    "for i in range(len(train_datasets)):\n",
    "    set_filename = train_datasets[i]\n",
    "    with open(set_filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    print(\"글자 {} 에 대한 트레이닝 데이터 개수는 {} 개입니다.\".format(os.path.basename(set_filename)[0], len(dataset)))\n",
    "\n",
    "print(\"\\n==== 테스트셋을 위한 데이터들 =====\")\n",
    "for i in range(len(test_datasets)):\n",
    "    set_filename = test_datasets[i]\n",
    "    with open(set_filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    print(\"글자 {} 에 대한 테스트 데이터 개수는 {} 개입니다.\".format(os.path.basename(set_filename)[0], len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 글자 별로 나뉘어져있던 데이터를 합쳐서 트레이닝셋, 테스트셋 2개로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(pickle_files, dataset_size):\n",
    "    num_classes = len(pickle_files)\n",
    "    dataset = np.ndarray((dataset_size, image_size, image_size), dtype=np.float32)\n",
    "    labels = np.ndarray(dataset_size, dtype=np.int32)\n",
    "    tsize_per_class = dataset_size // num_classes\n",
    "\n",
    "    start_t = 0\n",
    "    end_t = tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            letter_set = pickle.load(f)\n",
    "            np.random.shuffle(letter_set)\n",
    "\n",
    "            letter = letter_set[0:tsize_per_class, :, :]\n",
    "            dataset[start_t:end_t, :, :] = letter\n",
    "            labels[start_t:end_t] = label\n",
    "            start_t += tsize_per_class\n",
    "            end_t += tsize_per_class\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "train_size = 200000\n",
    "test_size = 10000\n",
    "\n",
    "train_dataset, train_labels = merge_datasets(train_datasets, train_size)\n",
    "test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 섞어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = shuffle(train_dataset, train_labels)\n",
    "test_dataset, test_labels = shuffle(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 섞은 후에 다시 한 번 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row = 2\n",
    "Column = 5\n",
    "ListOfLabel = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "\n",
    "print(\"==== 트레이닝 데이터 예시 ====\")\n",
    "images = train_dataset[0:10]\n",
    "labels = train_labels[0:10]\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(Row, Column, i+1)\n",
    "    plt.title('Label = {}'.format(ListOfLabel[labels[i]]))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"==== 테스트 데이터 예시 ====\")\n",
    "images = test_dataset[0:10]\n",
    "labels = test_labels[0:10]\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(Row, Column, i+1)\n",
    "    plt.title('Label = {}'.format(ListOfLabel[labels[i]]))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파일로 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "f = open(pickle_file, 'wb')\n",
    "save = {\n",
    "    'train_dataset': train_dataset[:50000,],\n",
    "    'train_labels': train_labels[:50000,],\n",
    "    'test_dataset': test_dataset[:5000,],\n",
    "    'test_labels': test_labels[:5000,],\n",
    "}\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
